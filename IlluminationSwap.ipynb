{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model based on \"Single Image Portrait Relighting\" whose aim is to produce same scenes as given with illumination conditions (color/light direction) swapped between 2 given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoding(nn.Module):\n",
    "    def __init__(self, in_channels=3, enc_channels=32, out_channels=64):\n",
    "        super(ImageEncoding, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, enc_channels - in_channels, 7, padding=3)\n",
    "        self.conv2 = nn.Conv2d(enc_channels, out_channels, 3, stride=2, padding=1)\n",
    "        self.encoded_image = None\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = self.conv1(image)\n",
    "        self.encoded_image = torch.cat((x, image), dim=1)  # append image channels to the convolution result\n",
    "        return self.conv2(self.encoded_image)\n",
    "    \n",
    "    def get_encoded_image(self):\n",
    "        # should not require tensor copying (based on https://github.com/milesial/Pytorch-UNet)\n",
    "        return self.encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownDoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, channels_per_group=16):\n",
    "        super(DownDoubleConv, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(in_channels // channels_per_group, in_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(out_channels // channels_per_group, out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.skip_connections = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.skip_connections.append(x)\n",
    "        x = self.conv1(x)\n",
    "        self.skip_connections.append(x)\n",
    "        return self.conv2(x)\n",
    "    \n",
    "    def get_skip_connections(self):\n",
    "        return self.skip_connections\n",
    "    \n",
    "    def reset_skip_connections(self):\n",
    "        self.skip_connections = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBottleneckConv(nn.Module):\n",
    "    def __init__(self, in_channels, channels_per_group=16, envmap_H=16, envmap_W=32, depth=4):\n",
    "        expected_channels = envmap_H * envmap_W\n",
    "        assert in_channels == expected_channels, f'DownBottleneck input has {in_channels} channels, expected {expected_channels}'\n",
    "        super(DownBottleneckConv, self).__init__()\n",
    "        self.convolutions = self._build_bottleneck_convolutions(in_channels, channels_per_group, depth)\n",
    "        self.skip_connections = []\n",
    "    \n",
    "    def _build_bottleneck_convolutions(self, in_channels, channels_per_group, depth):\n",
    "        single_conv = [nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(in_channels // channels_per_group, in_channels),\n",
    "            nn.PReLU()\n",
    "        )]\n",
    "        convolutions = single_conv * (depth - 1)\n",
    "        \n",
    "        # final layer before weighted pooling\n",
    "        out_channels = 4*in_channels\n",
    "        convolutions.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(out_channels // channels_per_group, out_channels),\n",
    "            nn.Softplus()\n",
    "        ))\n",
    "        \n",
    "        return nn.ModuleList(convolutions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for module in self.convolutions:\n",
    "            self.skip_connections.append(x)\n",
    "            x = module(x)\n",
    "        \n",
    "        # last layer output is not used as skip_connections:\n",
    "        self.skip_connections.pop()\n",
    "            \n",
    "        # split x into environment map predictions and confidence\n",
    "        channels = x.size()[1]\n",
    "        split_point = 3 * (channels // 4)\n",
    "        envmap_predictions, confidence = x[:, :split_point], x[:, split_point:]\n",
    "        \n",
    "        return envmap_predictions, confidence\n",
    "       \n",
    "    def get_skip_connections(self):\n",
    "        return self.skip_connections\n",
    "    \n",
    "    def reset_skip_connections(self):\n",
    "        self.skip_connections = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedPooling, self).__init__()\n",
    "    \n",
    "    def forward(self, x, weights):\n",
    "        # TODO: multiplication with sum can be probably implemented as convolution with groups (according to some posts)\n",
    "        return (x * weights.repeat((1, 3, 1, 1))).sum(dim=(2, 3), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiling(nn.Module):\n",
    "    def __init__(self, size=16, in_channels=1536, out_channels=512, channels_per_group=16):\n",
    "        super(Tiling, self).__init__()\n",
    "        self.size = size\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(out_channels // channels_per_group, out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tiled = x.repeat((1, 1, self.size, self.size))\n",
    "        return self.encode(tiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles on transposed convolutions:\n",
    "* [Convolution types](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)\n",
    "* [Upsampling with transposed convolution](https://medium.com/activating-robotic-minds/up-sampling-with-transposed-convolution-9ae4f2df52d0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_concat(x, y):\n",
    "    return torch.cat((x, y), dim=1)\n",
    "\n",
    "class UpBottleneckConv(nn.Module):\n",
    "    def __init__(self, in_channels, channels_per_group=16, envmap_H=16, envmap_W=32, depth=4):\n",
    "        expected_channels = envmap_H * envmap_W\n",
    "        assert depth >= 2, f'Depth should be not smaller than 3'\n",
    "        assert in_channels == expected_channels, f'UpBottleneck input has {in_channels} channels, expected {expected_channels}'\n",
    "        super(UpBottleneckConv, self).__init__()\n",
    "        self.depth = depth\n",
    "        \n",
    "        half_in_channels = in_channels // 2\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, half_in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(half_in_channels // channels_per_group, half_in_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        # TODO: why are these paddings necessary\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels + half_in_channels, in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(in_channels // channels_per_group, in_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.convs = nn.ModuleList([nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*in_channels, in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(in_channels // channels_per_group, in_channels),\n",
    "            nn.PReLU()\n",
    "        )] * (depth - 3))\n",
    "        # TODO: output_padding added to fit the spatial dimensions, but there is no reasoned justification for it\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*in_channels, half_in_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(half_in_channels // channels_per_group, half_in_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skip_connections):      \n",
    "        # encoding convolution\n",
    "        x = self.encode(x)\n",
    "        \n",
    "        # transposed convolutions with skip connections\n",
    "        x = self.initial_conv(channel_concat(x, skip_connections.pop()))\n",
    "        for conv in self.convs:\n",
    "            x = conv(channel_concat(x, skip_connections.pop()))\n",
    "        return self.out_conv(channel_concat(x, skip_connections.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpDoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, channels_per_group=16):\n",
    "        super(UpDoubleConv, self).__init__()\n",
    "        # TODO: why are these paddings necessary\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*in_channels, in_channels, 3, padding=1),\n",
    "            nn.GroupNorm(in_channels // channels_per_group, in_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*in_channels, out_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.GroupNorm(out_channels // channels_per_group, out_channels),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, skip_connections):\n",
    "        x = self.conv1(channel_concat(x, skip_connections.pop()))\n",
    "        return self.conv2(channel_concat(x, skip_connections.pop()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=3):\n",
    "        super(Output, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 3),  # should it be conv or transposed conv?\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, encoded_img):\n",
    "        return self.block(channel_concat(x, encoded_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated modules for skip connections management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Down, self).__init__()\n",
    "        self.image_encoder = ImageEncoding()\n",
    "        \n",
    "        # down\n",
    "        n_double_conv = 3\n",
    "        self.down_double_convs = self._build_down_double_convs(n_double_conv)\n",
    "        self.down_bottleneck = DownBottleneckConv(512)\n",
    "        self.weighted_pool = WeightedPooling()\n",
    "        \n",
    "        self.skip_connections = []\n",
    "    \n",
    "    def _build_down_double_convs(self, n):\n",
    "        return nn.ModuleList([DownDoubleConv(64*(2**i), 64*(2**(i+1))) for i in range(n)])\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # initial image encoding\n",
    "        x = self.image_encoder(image)\n",
    "        self.skip_connections.append(self.image_encoder.get_encoded_image())\n",
    "        \n",
    "        # double convolution layers\n",
    "        for down_double_conv in self.down_double_convs:\n",
    "            x = down_double_conv(x)\n",
    "            self.skip_connections += down_double_conv.get_skip_connections()\n",
    "            \n",
    "        # pre-bottleneck layer\n",
    "        env_map, weights = self.down_bottleneck(x)\n",
    "        self.skip_connections += self.down_bottleneck.get_skip_connections()\n",
    "        \n",
    "        # predict environment map\n",
    "        pred_env_map = self.weighted_pool(env_map, weights)\n",
    "        \n",
    "        return pred_env_map\n",
    "    \n",
    "    def get_skip_connections(self):\n",
    "        return self.skip_connections\n",
    "    \n",
    "    def reset_skip_connections(self):\n",
    "        self.skip_connections = []\n",
    "        for down_double_conv in self.down_double_convs:\n",
    "            down_double_conv.reset_skip_connections()\n",
    "        self.down_bottleneck.reset_skip_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Up, self).__init__()\n",
    "        \n",
    "        # up\n",
    "        n_double_conv = 3\n",
    "        self.tiling = Tiling()\n",
    "        self.up_bottleneck = UpBottleneckConv(512)\n",
    "        self.up_double_convs = self._build_up_double_convs(n_double_conv)\n",
    "        \n",
    "        self.output = Output()\n",
    "    \n",
    "    def _build_up_double_convs(self, n):\n",
    "        return nn.ModuleList([UpDoubleConv(256//(2**i), 256//(2**(i+1))) for i in range(n)])\n",
    "    \n",
    "    def forward(self, latent, skip_connections):\n",
    "        # tiling and channel reduction\n",
    "        tiled = self.tiling(latent)\n",
    "        \n",
    "        # post-bottleneck layer\n",
    "        x = self.up_bottleneck(tiled, skip_connections)\n",
    "        \n",
    "        # double convolution layers\n",
    "        for up_double_conv in self.up_double_convs:\n",
    "            x = up_double_conv(x, skip_connections)\n",
    "        \n",
    "        # final layer constructing image\n",
    "        relighted = self.output(x, skip_connections.pop())\n",
    "        \n",
    "        return relighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IlluminationSwapNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IlluminationSwapNet, self).__init__()\n",
    "        self.down = Down()\n",
    "        self.up = Up()\n",
    "    \n",
    "    def forward(self, image, target):\n",
    "        # pass image through encoder\n",
    "        image_env_map = self.down(image)\n",
    "        image_skip_connections = self.down.get_skip_connections()\n",
    "        self.down.reset_skip_connections()\n",
    "        \n",
    "        # pass target through encoder\n",
    "        target_env_map = self.down(target)\n",
    "        target_skip_connections = self.down.get_skip_connections()\n",
    "        self.down.reset_skip_connections()\n",
    "        \n",
    "        # decode image with target env map\n",
    "        image_relighted = self.up(target_env_map, image_skip_connections)\n",
    "        # can also add target relighting here\n",
    "        \n",
    "        return image_relighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for size mismatch errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IlluminationSwapNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(4, 3, 256, 256)\n",
    "target = torch.randn(4, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model(image, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
